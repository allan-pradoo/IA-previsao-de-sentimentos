{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Pa√ßos para utiliza√ß√£o:**\n",
        "\n",
        "\n",
        "1.   Instalar todas as bibliotecas que foram utilizadas;\n",
        "2.   Colocar o diretorio correto do banco de dados;\n",
        "3.   Rodas todas as c√©lulas do colab e aguardar;\n",
        "4.   Ap√≥s o treinamento ser√° poss√≠vel fazer a previs√£o utilizando predict_sentiment('*frase que deseja prever o sentimento*').\n",
        "\n",
        "\n",
        "# **Obrserva√ß√µes Importantes**:\n",
        "\n",
        "\n",
        "\n",
        "1. Na entrega final n√£o ser√° necess√°rio o treinamento da IA\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HoijyBgWWRS_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlzN0UmEOn95"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-text\n",
        "import tensorflow as tf\n",
        "import tensorflow_text"
      ],
      "metadata": {
        "id": "NLkPMX8-Qy18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pr√©-Processamento"
      ],
      "metadata": {
        "id": "sM-ru-WBX2TF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Carregar Dados**"
      ],
      "metadata": {
        "id": "P9naM35NX_MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YX01QbOsWcKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['sentiment' , 'text']"
      ],
      "metadata": {
        "id": "gLpE18btlQ_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/BERT/dataset_sentimentos_expandido (1).csv',\n",
        "                   header = None,\n",
        "                   names = cols,\n",
        "                   engine='python',\n",
        "                   encoding='utf-8')"
      ],
      "metadata": {
        "id": "7rGQY6W-YZ9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "XymrowfJYpS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(0)"
      ],
      "metadata": {
        "id": "GMeq5e9Rlnkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "3Zv20qSmmiBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "rXGyy3E2Z36-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "fD3OFI72aMSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail()"
      ],
      "metadata": {
        "id": "kzVTST1ZaOYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "JnDy2XG26miO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Limpeza de Texto**"
      ],
      "metadata": {
        "id": "ASZ1iu8lbDgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy emoji\n",
        "from ftfy import fix_text\n",
        "import emoji\n",
        "\n"
      ],
      "metadata": {
        "id": "AuF6nX4baY2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "    # Corrige caracteres quebrados e entidades HTML\n",
        "    tweet = fix_text(tweet)\n",
        "\n",
        "    # Remove men√ß√µes e URLs\n",
        "    tweet = re.sub(r\"@\\w+\", \" \", tweet)\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+\", \" \", tweet)\n",
        "\n",
        "    # Converte emojis em texto (ex: üòä -> :smiling_face:)\n",
        "    tweet = emoji.demojize(tweet)\n",
        "\n",
        "    # Transforma para min√∫sculas\n",
        "    tweet = tweet.lower()\n",
        "\n",
        "    # Remove acentos\n",
        "    tweet = unicodedata.normalize('NFKD', tweet).encode('ASCII', 'ignore').decode('utf-8', 'ignore')\n",
        "\n",
        "    # Remove caracteres n√£o alfab√©ticos exceto pontua√ß√£o relevante\n",
        "    tweet = re.sub(r\"[^a-zA-Z\\s!?']\", \" \", tweet)\n",
        "\n",
        "    # Normaliza repeti√ß√µes de letras (amoooo ‚Üí amo)\n",
        "    tweet = re.sub(r'(.)\\1{2,}', r'\\1\\1', tweet)\n",
        "\n",
        "    # Remove m√∫ltiplos espa√ßos\n",
        "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
        "\n",
        "    return tweet\n"
      ],
      "metadata": {
        "id": "hxit_X10daxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = '99 ' + data.text[0]\n",
        "test"
      ],
      "metadata": {
        "id": "9p1dPnVscbJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = clean_tweet(test)\n",
        "result"
      ],
      "metadata": {
        "id": "j4HBG9w-c86L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_clean = [clean_tweet(tweet) for tweet in data.text]"
      ],
      "metadata": {
        "id": "XlBYl99vdFi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_clean[0:4]"
      ],
      "metadata": {
        "id": "dvsNHbDgeQMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_labels = data.sentiment.values\n",
        "data_labels"
      ],
      "metadata": {
        "id": "zBBiMl4bexz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_labels"
      ],
      "metadata": {
        "id": "JQ0h-95ze2if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokeniza√ß√£o**"
      ],
      "metadata": {
        "id": "NVZsLVale5Fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow tensorflow_hub tensorflow_text\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForPreTraining\n",
        "from transformers import AutoModel"
      ],
      "metadata": {
        "id": "UzpYZz7QugwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, TFBertModel\n",
        "import unicodedata\n",
        "import re\n",
        "import emoji\n",
        "import os\n",
        "import keras\n",
        "from keras.saving import register_keras_serializable"
      ],
      "metadata": {
        "id": "NS6wBJwQsS9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Carregando tokenizer e modelo BERTimbal...\")\n",
        "MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
        "\n",
        "MAX_LENGTH = 64\n",
        "BATCH_SIZE = 16\n",
        "NB_CLASSES = 6"
      ],
      "metadata": {
        "id": "_E41QL10saMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentence_bert(sentence):\n",
        "    encoded = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        add_special_tokens=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "    return encoded['input_ids'][0], encoded['attention_mask'][0]\n",
        "\n",
        "print(\"Codificando senten√ßas...\")\n",
        "bert_inputs_encoded = [encode_sentence_bert(sentence) for sentence in data_clean]\n",
        "\n",
        "all_input_ids = tf.stack([pair[0] for pair in bert_inputs_encoded])\n",
        "all_attention_masks = tf.stack([pair[1] for pair in bert_inputs_encoded])\n",
        "all_labels = tf.convert_to_tensor(data_labels, dtype=tf.int32)\n",
        "\n",
        "print(f\"Total de exemplos: {len(data_labels)}\")\n",
        "print(f\"Shape dos Input IDs: {all_input_ids.shape}\")\n",
        "print(f\"Shape das Attention Masks: {all_attention_masks.shape}\")\n",
        "print(f\"Shape dos Labels: {all_labels.shape}\")\n",
        "\n",
        "\n",
        "bert_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    ((all_input_ids, all_attention_masks), all_labels)\n",
        ")\n",
        "\n",
        "\n",
        "BUFFER_SIZE = len(data_labels)\n",
        "bert_dataset = bert_dataset.shuffle(BUFFER_SIZE)\n",
        "\n",
        "\n",
        "bert_dataset = bert_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "NB_BATCHES = len(data_labels) // BATCH_SIZE\n",
        "if len(data_labels) % BATCH_SIZE != 0:\n",
        "    NB_BATCHES += 1\n",
        "NB_BATCHES_TEST = NB_BATCHES // 10\n",
        "if NB_BATCHES_TEST == 0 and NB_BATCHES > 0:\n",
        "     NB_BATCHES_TEST = 1\n",
        "\n",
        "print(f\"Total de batches: {NB_BATCHES}\")\n",
        "print(f\"Batches de teste: {NB_BATCHES_TEST}\")\n",
        "print(f\"Batches de treino: {NB_BATCHES - NB_BATCHES_TEST}\")\n",
        "\n",
        "test_dataset = bert_dataset.take(NB_BATCHES_TEST)\n",
        "train_dataset = bert_dataset.skip(NB_BATCHES_TEST)"
      ],
      "metadata": {
        "id": "pWwKZnvdsfkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTimbauClassifier(tf.keras.Model):\n",
        "    def __init__(self, nb_classes=NB_CLASSES, dropout_rate=0.3, model_name=MODEL_NAME):\n",
        "        super(BERTimbauClassifier, self).__init__(name=\"BERTimbauClassifier\")\n",
        "        self.bert = TFBertModel.from_pretrained(model_name)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "        if nb_classes == 2:\n",
        "            self.classifier = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')\n",
        "        else:\n",
        "            self.classifier = tf.keras.layers.Dense(nb_classes, activation='softmax', name='classifier')\n",
        "\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        input_ids, attention_mask = inputs\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, training=training)\n",
        "        pooled_output = bert_output.pooler_output\n",
        "        x = self.dropout(pooled_output, training=training)\n",
        "        return self.classifier(x)\n",
        "\n",
        "print(\"Instanciando e compilando o modelo...\")\n",
        "model = BERTimbauClassifier(nb_classes=NB_CLASSES, dropout_rate=0.3)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "\n",
        "if NB_CLASSES == 2:\n",
        "    loss_function = 'binary_crossentropy'\n",
        "    metrics_list = ['accuracy']\n",
        "else:\n",
        "    loss_function = 'sparse_categorical_crossentropy'\n",
        "    metrics_list = ['sparse_categorical_accuracy']\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=loss_function,\n",
        "              metrics=metrics_list)\n",
        "\n",
        "try:\n",
        "    sample_batch_inputs, _ = next(iter(train_dataset))\n",
        "    model(sample_batch_inputs)\n",
        "    model.summary()\n",
        "except StopIteration:\n",
        "    print(\"N√£o foi poss√≠vel gerar um batch de exemplo para model.summary() (dataset de treino vazio?).\")\n",
        "\n",
        "\n",
        "checkpoint_dir = '/content/drive/MyDrive/BERT' #coloque o caminho que ser√° salvo o checkpoint\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "checkpoint_path = os.path.join(checkpoint_dir, 'ckpt')\n",
        "\n",
        "ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_dir, max_to_keep=1)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()\n",
        "    print(f'√öltimo checkpoint restaurado de {ckpt_manager.latest_checkpoint}')\n",
        "else:\n",
        "    print(\"Nenhum checkpoint encontrado, iniciando treinamento do zero.\")\n",
        "\n",
        "\n",
        "class MyCustomCallBack(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        save_path = ckpt_manager.save()\n",
        "        print(\"\\nCheckpoint salvo para a √©poca {} em {}\".format(epoch + 1, save_path))\n",
        "\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "print(f\"\\nIniciando treinamento por {EPOCHS} √©pocas...\")\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=EPOCHS,\n",
        "                    callbacks=[MyCustomCallBack()],\n",
        "                    validation_data=test_dataset)\n",
        "print(\"\\nTreinamento conclu√≠do!\")\n",
        "\n",
        "if NB_BATCHES_TEST > 0:\n",
        "    print(\"\\nAvaliando o modelo no conjunto de teste...\")\n",
        "    loss, accuracy = model.evaluate(test_dataset, verbose=0)\n",
        "    print(f\"Loss no Teste: {loss:.4f}\")\n",
        "    if NB_CLASSES == 2:\n",
        "         print(f\"Acur√°cia no Teste: {accuracy:.4f}\")\n",
        "    else:\n",
        "         print(f\"Acur√°cia Categ√≥rica Esparsa no Teste: {accuracy:.4f}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNenhum dado de teste para avalia√ß√£o final.\")\n"
      ],
      "metadata": {
        "id": "9ykZMOystVx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_specific_label_map = {\n",
        "    0: 'Satisfa√ß√£o',\n",
        "    1: 'Frustra√ß√£o',\n",
        "    2: 'Confus√£o',\n",
        "    3: 'Urg√™ncia/Press√£o',\n",
        "    4: 'Raiva/Irrita√ß√£o',\n",
        "    5: 'Neutro'\n",
        "}\n",
        "\n",
        "def predict_sentiment(sentence):\n",
        "\n",
        "    try:\n",
        "        cleaned_sentence = clean_tweet(sentence)\n",
        "    except NameError:\n",
        "        print(\"Aviso: Fun√ß√£o clean_tweet n√£o encontrada. Usando frase original.\")\n",
        "        cleaned_sentence = sentence\n",
        "\n",
        "    encoded = tokenizer.encode_plus(\n",
        "        cleaned_sentence,\n",
        "        add_special_tokens=True,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    input_ids = encoded['input_ids']\n",
        "    attention_mask = encoded['attention_mask']\n",
        "\n",
        "\n",
        "    output_probabilities = model((input_ids, attention_mask), training=False)[0]\n",
        "\n",
        "\n",
        "    predicted_class_index = tf.argmax(output_probabilities).numpy()\n",
        "\n",
        "    predicted_sentiment_label = user_specific_label_map.get(predicted_class_index, f\"Classe Desconhecida ({predicted_class_index})\")\n",
        "\n",
        "    print(f\"Sentimento previsto: {predicted_sentiment_label}\")\n",
        "\n",
        "print(\"\\n--- Testando a Previs√£o de Sentimento ---\")\n"
      ],
      "metadata": {
        "id": "l5CW7mz6GLSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment(\"Estou aguardando h√° muito tempo e ningu√©m responde.\")"
      ],
      "metadata": {
        "id": "meGYmNXEGV-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment('J√° tentei v√°rias vezes e nunca consigo acessar minha conta.')"
      ],
      "metadata": {
        "id": "kIqcBJzTGcKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment('Estou muito contente com o resultado final, superou minhas expectativas!')"
      ],
      "metadata": {
        "id": "3-KnWar1G4Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment('Esse servi√ßo √© uma piada, estou indignado')"
      ],
      "metadata": {
        "id": "7a-eUv-EG74g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment(\"Eu n√£o sei oq isso significa\")"
      ],
      "metadata": {
        "id": "nbK1shO_HeQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment('Meu cart√£o venceu, como solicito um novo?')"
      ],
      "metadata": {
        "id": "B7CDaFitHiy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment(\"N√£o entendi o procedimento descrito, ficou muito confuso.\")"
      ],
      "metadata": {
        "id": "DXB_DXpDHrks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment(\"O procedimento foi realizado conforme o cronograma previamente estabelecido, sem intercorr√™ncias significativas ou altera√ß√µes dignas de nota.\")"
      ],
      "metadata": {
        "id": "MLszUwGiHuzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment('O evento ocorrer√° conforme planejado.')"
      ],
      "metadata": {
        "id": "DbMJbQQxH2zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment('Precisamos agir agora, √© uma quest√£o de minutos!')"
      ],
      "metadata": {
        "id": "rHOflqV_H4IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment('Voc√™ poderia me ajudar nesse caso?')"
      ],
      "metadata": {
        "id": "1lWW-vhRIBIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment('J√° perdi a paci√™ncia, √© um descaso total.')"
      ],
      "metadata": {
        "id": "V8wkP6DnIFv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "for batch in test_dataset:\n",
        "    (input_ids, attention_mask), labels = batch\n",
        "    logits = model((input_ids, attention_mask), training=False)\n",
        "    predictions = tf.argmax(logits, axis=1)\n",
        "\n",
        "    true_labels.extend(labels.numpy())\n",
        "    predicted_labels.extend(predictions.numpy())\n",
        "\n",
        "class_indices = [0, 1, 2, 3, 4, 5]\n",
        "\n",
        "class_names = ['Satisfa√ß√£o', 'Frustra√ß√£o', 'Confus√£o', 'Urg√™ncia/Press√£o', 'Raiva/Irrita√ß√£o', 'Neutro']\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels, labels=class_indices)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu', xticklabels=class_names, yticklabels=class_names, linewidths=0.5)\n",
        "plt.xlabel('Predito')\n",
        "plt.ylabel('Verdadeiro')\n",
        "plt.title('Matriz de Confus√£o')\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(true_labels, predicted_labels, target_names=class_names, labels=class_indices))\n"
      ],
      "metadata": {
        "id": "ENdEKSmcIaIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avalia√ß√£o"
      ],
      "metadata": {
        "id": "J3jKGYTr5ghV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history.history.keys()"
      ],
      "metadata": {
        "id": "ioV0GN7IfRDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.title('Loss progress');"
      ],
      "metadata": {
        "id": "27rf0ovJ5iPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['sparse_categorical_accuracy'])\n",
        "plt.title('Accuracy progress');"
      ],
      "metadata": {
        "id": "hUGTJKJl5kSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_dataset)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "aObn9otG5njV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PqQkZBArf6LN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}